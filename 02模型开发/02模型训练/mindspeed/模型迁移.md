# MindSpeed-LLM: Qwen2.5 模型迁移

- 第1章 概述
- 第2章 模型迁移总体流程
- 第3章 模型元数据解读
- 第4章 模型适配
- 第5章 权重转换适配
- 第6章 精度调试

## 概述

随着LLM模型的不断发展，性能优异结构创新的大规模预训练模型不断涌现，这些开源模型会被发布在开源社区上，例如huggleface，提供以transformer权重，但要在我们的框架上顺利跑起来，乃至性能正常、精度对齐，就需要完成模型迁移适配：通过比对目标模型的模型结构，抓取其模型特性，并将其特性结构适配到MindSpeed-LLM中，从而使MindSpeed-LLM与开源模型结构对齐，使权重和输入数据做相同运算，确保输出对齐，同时保持或提升该模型的性能。

### 什么是模型迁移模型
迁移是指将原本运行在GPU或其他硬件平台的深度学习模型迁移至NPU，并保障模型在合理精度误差范围内高性能运行。
### 为什么要做模型迁移
将模型从其他硬件平台迁移到NPU时，由于软件架构和三方库的不同，涉及到一系列底层到上层的适配操作。以GPU为例，模型迁移至NPU需要适配的原因可分为三方面：
- 硬件特性和性能特点差异

  由于NPU和GPU的硬件特性和性能特点不同，模型在NPU上可能需要进一步的性能调试和优化，以充分发挥NPU的潜力。
- 计算架构差异

  CUDA（Compute Unified Device Architecture）+ cuDNN（CUDA Deep Neural Network）是NVIDIA GPU的并行计算架构，而CANN（Compute Architecture for Neural Networks）是华为NPU的异构计算架构。
- 深度学习框架差异

  为了支持NPU硬件，MindSpeed 对 Megatron-LM进行封装适配：包括适配张量运算、自动微分等功能，以便在NPU上高效执行，MindSpeed-LLM则是借用MindSpeed实现Megatron-LM和NPU的兼容。


## 模型迁移总体流程
我们的目标是确保模型在相同输入的前提下，使目标模型在MindSpeed-LLM和开源模型框架间，模型结构等价，并且使其权重可以在这些框架间可以正常转换，从而保证其最终的精度对齐。 

据此，我们将整个模型迁移任务分为如下几个步骤：

- [模型元数据解读](#模型元数据解读)
  - 从开源社区下载开源模型源文件，通过查看其开源模型结构定义文件，了解模型特性。
- [模型适配](#模型适配)
  - 熟悉MindSpeed-LLM模型如何加载权重并运行模型，了解其中模型组成部分，如embedding、self-attention、mlp等模型结构的具体定义和正反向逻辑处理。
  - 比较解析的模型元数据中的特性结构和当前模型的定义区别，进行特性适配，使该模块运行的初始化和运行逻辑对齐。
- [权重转换适配](#权重转换适配)
  - 不同模型可能有自定义的特性名称，但对应的模型结构是同一种，根据开源模型源文件的特性定义，完成特性和权重定义名称的映射对齐。
  - 查看开源模型源文件的权重结构组成，将其模块层逐层解析其数据组成，与MindSpeed-LLM提供的接口对齐，完成权重的逐层对齐，完成加载。
  - MindSpeed-LLM对于不同模块的权重加载是有明确定义的，通过解读代码了解其具体的权重格式要求，完成转换权重和存储。
- [精度调试](#精度调试)
  - 编写权重转换脚本，实现hf2mcore的转换，确认功能正常。
  - 编写权重转换脚本，实现mcore2hf的转换，确认功能正常，且通过张量比较，确认与原始权重的张量相同，无损转换。
  - 元模型与适配模型都加载权重，进行前向对齐，逐层比对确认前向对齐。
  - 为适配模型编写脚本，将元模型运行在gpu上，加载权重运行长跑实验，比对是否正反向对齐，迭代一致。

## 模型元数据解读

当前常用的开源模型社区有如Huggingface、魔乐社区、魔搭社区、github等网站，所有开源的模型源文件都可以从以上社区获取。

如图所示为一份开源权重包含的内容：

![img_1.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/img_1.png)

- readme.md 为模型的介绍信息
- config.json表达了模型定义及参数
- safetensors为开源模型预训练权重
  - 配套model_safetensors.index.json记录了权重附带的层定义。
- 若干tokenizer文件: 记录该模型的词表信息。
- 个别模型会有modeling_{model}.py文件，其中记录了模型的明确定义。
  - 其他没有该文件的开源模型需要去transformer中找到对应的模型定义，详见下文config.json解读。

如下对这些文件内容进行解读。

### config.json解读
如下图所示为截取的Qwen2.5-7B模型的config.json，其中记录了模型的特性参数，下面对一些关键参数进行挑选解读。

![img_2.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/img_2.png)

#### 基础架构参数
- architectures	["Qwen2ForCausalLM"] 模型架构类名，表示这是一个语言模型
- model_type	"qwen2"	             模型类型标识符，用于 HuggingFace 自动加载对应的模型类
- torch_dtype	"bfloat16"	         模型默认张量数据类型（BF16 平衡精度和显存效率）

#### 模型结构参数
- hidden_size	    3584	             隐藏层维度（每层Transformer的向量维度）。
- num_hidden_layers	28	             Transformer 的总层数。
- num_attention_heads	28	             注意力head的总数（用于多头注意力）。
- num_key_value_heads	4	             Grouped Query Attention (GQA) 中 Key/Value 的共享头数。
- intermediate_size	18944	         FFN（前馈网络）中间层的维度（通常为 4×hidden_size）。
- hidden_act	        "silu"	         FFN 激活函数（SiLU/Swish，比 ReLU 更平滑）。
- rms_norm_eps	    1e-06	         RMSNorm 的极小值（防止除零错误）。
- tie_word_embeddings	false	         是否绑定embedding/output层（设为 false 时输出层独立训练）。

#### 位置编码与注意力
- max_position_embeddings	131072	     模型支持的最大序列长度。
- rope_theta	        1000000.0	     RoPE（旋转位置编码）的基础周期（控制远程衰减特性）。
- use_mrope	        false	         是否使用 动态混合 RoPE（如 YaRN 或动态 NTK）。
- sliding_window	    131072	         滑动窗口注意力（SWA）的窗口大小（若启用，局部注意力范围）。
- use_sliding_window	false	         是否实际启用滑动窗口注意力。
- attention_dropout	0.0	             注意力权重的 Dropout 概率（0 表示不丢弃）。

#### 其他
- vocab_size为152064，表示词表大小，能支持的总token数量。
- 如上的model_type表示该模型归属于transformer已支持的模型类别，qwen2，具体的模型定义可以去查阅transformers
  - 比如当前模型qwen2.5-7b, 可以去看[transformer仓库](https://github.com/huggingface/transformers)src/transformers/models/qwen2/modeling_qwen2.py 

### model_safetensors.index.json解读
如下图所示为截取的Qwen2.5-7B模型的safetensors.index.json，
- 为了高效存储和加载超大模型参数，通常会把一个大模型裁切成多份safetensors文件进行分片存储，便于并行加载和最小任务加载权重；
- 配套有一个model.safetensors.index.json文件，其中metadata记录了总参数字节数，并有一个weight_map，用于记录模块权重与safetensor文件之间的映射关系。

![img_4.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/img_4.png)

### tokenizer文件
tokenizer.json 和 tokenizer_config.json 是分词器（Tokenizer）的核心配置文件，二者分工明确，共同定义了文本如何转换为模型可处理的 token ID。

#### tokenizer.json
分词器的主规则文件，包含所有具体的分词逻辑和词表映射。由 HuggingFace 的 tokenizers 库生成，用于高效分词。如下是内容的概要介绍：
```shell
{
  "model": {
    "type": "BPE",                            // 分词算法（如 BPE、WordPiece、Unigram）
    "vocab": {"<pad>": 0, "hello": 1, ...},   // 词表（token → ID 的映射）
    "merges": ["h e", "e l", "l o"],          // BPE 算法的合并规则
    "unk_token": "<unk>"                      // 未知 token 的占位符
  },
  "normalizer": {"type": "NFKC"},             // 文本规范化规则（如统一 Unicode）
  "pre_tokenizer": {"type": "ByteLevel"},     // 预分词规则（如按空格分割）
  "post_processor": {                         // 后处理（如添加 [CLS]/[SEP]）
    "type": "TemplateProcessing",
    "single": "[CLS] $A [SEP]"
  }
}
```
#### tokenizer_config.json
分词器的辅助配置文件，定义分词器的行为参数和特殊 token。如下为概要内容：
```shell
{
  "tokenizer_class": "PreTrainedTokenizerFast",  // 分词器类名
  "bos_token": "<s>",                            // 序列开始 token
  "eos_token": "</s>",                           // 序列结束 token
  "unk_token": "<unk>",                          // 未知 token
  "pad_token": "<pad>",                          // 填充 token
  "model_max_length": 32768,                     // 最大输入长度
  "clean_up_tokenization_spaces": true,          // 是否清理多余空格
  "tokenizer_file": "tokenizer.json"             // 指向主规则文件
}
```

### modelling_{model}.py
modelling_{model}.py文件是 HuggingFace Transformers 库中实现模型架构的核心代码文件，定义了模型的计算逻辑、前向传播流程以及与Transformers框架的集成接口。对于当前示例模型qwen2.5-7b模型，其modelling_qwen2.py由config.json索引到transformers库中，以下对该文件进行详细解析：

```shell
# 如下为 类定义
__all__ = [
    "Qwen2PreTrainedModel",            
    "Qwen2Model",
    "Qwen2ForCausalLM",
    "Qwen2ForSequenceClassification",
    "Qwen2ForTokenClassification",
    "Qwen2ForQuestionAnswering",
]
```
这些类是基于 Qwen2 模型架构的不同任务变体，继承自基类"Qwen2PreTrainedModel"，用于支持不同类型的自然语言处理任务。
- Qwen2PreTrainedModel:  Qwen2 模型类的基类，继承自 PreTrainedModel, 提供模型加载和保存的通用逻辑, 初始化模型权重。
- Qwen2Model：Qwen2的基础Transformer模型结构，包含了backbone，是如下4个类的主体，其中定义了模型的Embedding，TransformerLayers，RMSNorm等结构。
- Qwen2ForCausalLM: 用于因果语言建模（如文本生成、续写）,相较Qwen2Model增加一个lm_head， 用于将hidden_states映射到词表空间。
- Qwen2ForSequenceClassification：用于序列级分类任务（如情感分析、文本分类），任务头为分类层（通常为 nn.Linear(hidden_size, num_labels)
- Qwen2ForTokenClassification：用于标记级分类任务（如命名实体识别、词性标注），任务头为分类层（每个 token 一个分类结果）
- Qwen2ForQuestionAnswering：用于问答任务，两个线性层（分别预测答案起始和结束位置）

![img_5.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/img_5.png)

Qwen2Model作为其模型主体，是我们关注的重点，需要与其做好模块的结构和输入输出的对齐，完成模型迁移。如下所示为其初始化定义，并且声明了其Embedding、layers、Qwen2RMSNorm等组成部分，详细定义请自行研读了解。

```shell
# 基础 Transformer 结构（多层堆叠）
class Qwen2Model(Qwen2PreTrainedModel):
    """
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Qwen2DecoderLayer`]

    Args:
        config: Qwen2Config
    """

    def __init__(self, config: Qwen2Config):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [Qwen2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen2RotaryEmbedding(config=config)
        self.gradient_checkpointing = False

        # Initialize weights and apply final processing
        self.post_init()
```

简单示例：
```shell
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig

load_dir = "/your/hf_weights/Qwen2.5-7B-hf"
config = AutoConfig.from_pretrained(load_dir, trust_remote_code=True)
hf_model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)
print(hf_model)

# 在此基础上，可以通过pdb，进行直观解读模型结构
```
![img_6.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/img_6.png)

## 模型适配

如图所示为模型适配的大体流程图：

![img_3.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/img_3.png)

### 模型结构解析

![img_7.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/img_7.png)

根据模型结构，我们将其分为如下几个组成部分，下面进行代码模块段一一映射比对,比对transformer中[modeling_qwen2.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2/modeling_qwen2.py)
以及Megatron-LM core分支[gpt_model.py](https://github.com/NVIDIA/Megatron-LM/blob/core_r0.8.0/megatron/core/models/gpt/gpt_model.py)

MindSpeed-LLM是基于Megatron-LM改进的NPU大语言模型优化库，Megatron-LM相较Transformer多了很多分布式考虑的操作。
#### Embedding
```shell
# transformer，见class Qwen2Model的self.embed_tokens 
self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)

# forward时
# 将输入的tokens转换为embedding向量。
if inputs_embeds is None:
    inputs_embeds = self.embed_tokens(input_ids)
# 初始化KV Cache，标记当前输入token的绝对位置编码
if use_cache and past_key_values is None: 
    past_key_values = DynamicCache() 
if cache_position is None: 
    past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    cache_position = torch.arange(
        past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    )
if position_ids is None:
    position_ids = cache_position.unsqueeze(0)
# 生成注意力掩码
causal_mask = self._update_causal_mask(
    attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions
)
```
```shell
# 对应Megatron-LM，见class GPTModel的 self.embedding
self.embedding = LanguageModelEmbedding(
    config=self.config,
    vocab_size=self.vocab_size,
    max_sequence_length=self.max_sequence_length,
    position_embedding_type=position_embedding_type,
)
  
# forward时
decoder_input = self.embedding(input_ids=input_ids, position_ids=position_ids)
# 该步的作用负责将输入的tokens转换为嵌入向量（Embeddings），并处理与分布式训练、序列并行相关的逻辑。
# 将输入的tokens转换为embedding向量。
word_embeddings = self.word_embeddings(input_ids)
# 为输入tokens添加位置信息（对比Transformer中的绝对位置编码）
if self.add_position_embedding:
    position_embeddings = self.position_embeddings(position_ids)
    embeddings = word_embeddings + position_embeddings
else:
    embeddings = word_embeddings
# 调整张量格式以适应Megatron-LM的分布式计算，更高效。
if not self.reduce_scatter_embeddings:
    # Data format change to avoid explicit tranposes : [b s h] --> [s b h].
    embeddings = embeddings.transpose(0, 1).contiguous()
# Token 类型嵌入，本用例未使用
if tokentype_ids is not None:
    assert self.tokentype_embeddings is not None
    # [b s h] -> [s b h] (So that it can be added with embeddings)
    tokentype_embedding = self.tokentype_embeddings(tokentype_ids).permute(1, 0, 2)
    embeddings = embeddings + tokentype_embedding
else:
    assert self.tokentype_embeddings is None
# 浮点精度处理，确保残差连接的数值稳定性
if self.config.fp32_residual_connection:
    embeddings = embeddings.float()

# 在序列并行下，将长序列分块到多个NPU上计算。
if self.config.sequence_parallel:
    if not self.reduce_scatter_embeddings:
        embeddings = tensor_parallel.scatter_to_sequence_parallel_region(embeddings)
    # `scatter_to_sequence_parallel_region` returns a view, which prevents
    # the original tensor from being garbage collected. Clone to facilitate GC.
    # Has a small runtime cost (~0.5%).
    if self.config.clone_scatter_output_in_embedding:
        embeddings = embeddings.clone()
    with tensor_parallel.get_cuda_rng_tracker().fork():
        embeddings = self.embedding_dropout(embeddings)
else:
    embeddings = self.embedding_dropout(embeddings)
```
基于如上比对，在不考虑分布式的处理，两者操作等价，后续类似的分布式操作代码将不再讲解，精度对齐时该部分内容也不参与计算。

#### RopeEmbedding
```shell
# transformer，见class Qwen2Model的self.rotary_emb 
self.rotary_emb = Qwen2RotaryEmbedding(config=config)
class Qwen2RotaryEmbedding(nn.Module):
    def __init__(self, config: Qwen2Config, device=None):
        super().__init__()
        # 根据config.json中的rope_type参量未定义，可知rope_type为default
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        # 重点关注这里的参量初始化，比如inv_freq, self.attention_scaling和max_seq_len_cached
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        # 此处的config对应 config.json，在rope_init_fn初始化时从其中获取参量。
        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        # 在default中
        # attention_factor = 1.0  # Unused in this type of RoPE
        # inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

# forward时
# 将位置信息（position_ids）通过旋转矩阵编码到hidden_states中，替代传统的绝对/相对位置嵌入。
position_embeddings = self.rotary_emb(hidden_states, position_ids)
# 如下为rope部分的前向
def forward(self, x, position_ids):
    inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
    position_ids_expanded = position_ids[:, None, :].float()

    device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
    with torch.autocast(device_type=device_type, enabled=False):  # Force float32
        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        emb = torch.cat((freqs, freqs), dim=-1)
        cos = emb.cos() * self.attention_scaling
        sin = emb.sin() * self.attention_scaling

    return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
```
```shell
# 对应Megatron-LM，见class GPTModel的 self.rotary_pos_emb
# 当设定position_embedding_type为rope时，完成初始化
if self.position_embedding_type == 'rope':
    self.rotary_pos_emb = RotaryEmbedding(
        kv_channels=self.config.kv_channels,
        rotary_percent=rotary_percent,
        rotary_interleaved=self.config.rotary_interleaved,
        seq_len_interpolation_factor=seq_len_interpolation_factor,
        rotary_base=rotary_base,
        use_cpu_initialization=self.config.use_cpu_initialization,
    )
    def __init__(
        self,
        kv_channels: int,
        rotary_percent: float,
        rotary_interleaved: bool = False,
        seq_len_interpolation_factor: float = None,
        rotary_base: int = 10000,
        use_cpu_initialization: bool = False,
    ) -> None:
        super().__init__()
        # 当kv_channels未设置时，会被计算为 self.kv_channels = self.hidden_size // self.num_attention_heads
        dim = kv_channels
        if rotary_percent < 1.0: # 默认rotary_percent为1.0
            dim = int(dim * rotary_percent)
        self.rotary_interleaved = rotary_interleaved

        self.seq_len_interpolation_factor = seq_len_interpolation_factor
        device = 'cpu' if use_cpu_initialization else torch.cuda.current_device()
        # 计算结果一致。
        self.inv_freq = 1.0 / (
            rotary_base ** (torch.arange(0, dim, 2, dtype=torch.float32, device=device) / dim)
        )

# forward时
if self.position_embedding_type == 'rope':
    # 根据其推理参数、并行状态，确定当前输入序列的实际长度，用于正确生成Rope位置索引。
    rotary_seq_len = self.rotary_pos_emb.get_rotary_seq_len(
        inference_params, self.decoder, decoder_input, self.config
    )
    rotary_pos_emb = self.rotary_pos_emb(rotary_seq_len)
# qwen2场景未设定seq_len_interpolation_factor，offset为0，rotary_interleaved=False
def forward(self, max_seq_len: int, offset: int = 0) -> Tensor:
    if self.inv_freq.device.type == 'cpu':
        # move `inv_freq` to GPU once at the first micro-batch forward pass
        self.inv_freq = self.inv_freq.to(device=torch.cuda.current_device())
    seq = (
        torch.arange(max_seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
        + offset
    )

    if self.seq_len_interpolation_factor is not None:
        seq *= 1 / self.seq_len_interpolation_factor

    freqs = torch.outer(seq, self.inv_freq)
    # first part even vector components, second part odd vector components,
    #  2 * dim in dimension size
    if not self.rotary_interleaved:
        emb = torch.cat((freqs, freqs), dim=-1)
    else:
        emb = torch.stack((freqs.view(-1, 1), freqs.view(-1, 1)), dim=-1).view(
            freqs.shape[0], -1
        )
    # emb [seq_length, .., dim]
    emb = emb[:, None, None, :]
    if parallel_state.get_context_parallel_world_size() > 1:
        # slice rotary_pos_emb along sequence dimension and select the parition of the current CP rank
        emb = get_pos_emb_on_this_cp_rank(emb, 0)
    return emb
```
虽然两者计算方法不同，但运行场景相同，可以进行等价的rope运算。

#### RMSNorm
```shell
# transformer，见class Qwen2Model的self.norm 
self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
class Qwen2RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        Qwen2RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)
```
```shell
# 因为NPU架构没有transformer_engine, MindSpeed-LLM对Megatron-LM这一部分做了Patch，
# patch见mindspeed_llm/tasks/megatron_adaptor.py::CoreAdaptation::patch_core_models
# 其中get_gpt_layer_local_spec_wrapper对RMSNorm替换为PTNorm
# PTNorm会根据输入的normalization取值["RMSNorm","LayerNorm"]
# PTNorm定义详见mindspeed_llm/core/transformer/custom_layers/transformer_engine.py
def get_gpt_layer_local_spec_wrapper(fn):
    @wraps(fn)
    def wrapper(num_experts: int = None, moe_grouped_gemm: bool = False, qk_layernorm: bool = False):
        res = fn(num_experts, moe_grouped_gemm, qk_layernorm)

        res.submodules.input_layernorm = PTNorm
        res.submodules.pre_mlp_layernorm = PTNorm

        if qk_layernorm:
            res.submodules.self_attention.submodules.q_layernorm = PTNorm
            res.submodules.self_attention.submodules.k_layernorm = PTNorm
        return res

    return wrapper
```
由于patch定义有所区别，需要通过输入参数控制，使MindSpeed-LLM与Transformer在运行时运行相同的逻辑。

#### SelfAttention
下面再展开讲一下Qwen2DecoderLayer，该部分除了上述提到的Qwen2RMSNorm，还有Qwen2Attention以及Qwen2MLP。这里对Attention部分做比对说明。
```shell
# transformer，见class Qwen2Model的self.norm 
class Qwen2Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: Qwen2Config, layer_idx: int):
        super().__init__()
        self.config = config
        # 参量初始化
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = True
        # qkv和output分别是一个线性层
        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)
        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)
        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)
        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)
        
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: Tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        sliding_window = None
        if (
            self.config.use_sliding_window
            and getattr(self.config, "sliding_window", None) is not None
            and self.layer_idx >= self.config.max_window_layers
        ):
            sliding_window = self.config.sliding_window

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            if self.config._attn_implementation == "sdpa" and kwargs.get("output_attentions", False):
                logger.warning_once(
                    "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to "
                    'eager attention. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'
                )
            else:
                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            sliding_window=sliding_window,  # main diff with Llama
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights
```
```shell
# Megatron-LM，见megatron/core/transformer/attention.py中的定义 
# ！！Qwen2Attention实际对应SelfAttention部分，此处截取部分代码说明其差异
class SelfAttention(Attention):
    def __init__(
        self,
        config: TransformerConfig,
        submodules: SelfAttentionSubmodules,
        layer_number: int,
        attn_mask_type=AttnMaskType.padding,
    ):
        ...
        # 注意到qwen2attention的q/k/v_proj带有bias=True，而o_proj的bias=False，表示其qkv有bias，o没有
        # 初始化注意，要使能add_qkv_bias
        self.linear_qkv = build_module(
            submodules.linear_qkv,
            self.config.hidden_size,
            self.query_projection_size + 2 * self.kv_projection_size,
            config=self.config,
            init_method=self.config.init_method,
            gather_output=False,
            bias=self.config.add_bias_linear or self.config.add_qkv_bias,
            skip_bias_add=False,
            is_expert=False,
            tp_comm_buffer_name='qkv',
        )
        ...
        
class Attention(MegatronModule, ABC):
    def __init__(
        self,
        config: TransformerConfig,
        submodules: Union[SelfAttentionSubmodules, CrossAttentionSubmodules],
        layer_number: int,
        attn_mask_type: AttnMaskType,
        attention_type: str,
    ):
        super().__init__(config=config)
        ...

        # 这里是走的core_attention对应transformer的attention_interface 
        self.core_attention = build_module(
            submodules.core_attention,
            config=self.config,
            layer_number=self.layer_number,
            attn_mask_type=self.attn_mask_type,
            attention_type=self.attention_type,
        )
        ...
        # linear_proj对应o_proj, 这里的self.config.add_bias_linear要设置为false
        self.linear_proj = build_module(
            submodules.linear_proj,
            self.query_projection_size,
            self.config.hidden_size,
            config=self.config,
            init_method=self.config.output_layer_init_method,
            bias=self.config.add_bias_linear,
            input_is_parallel=True,
            skip_bias_add=True,
            is_expert=False,
            tp_comm_buffer_name='proj',
        )

    def forward(
        self,
        hidden_states,
        attention_mask,
        key_value_states=None,
        inference_params=None,
        rotary_pos_emb=None,
        packed_seq_params=None,
    ):
        # 用于生成推理需要的rotary_pos_emb数据，也是k,v的位置编码信息。
        if rotary_pos_emb is not None and not isinstance(rotary_pos_emb, tuple):
            rotary_pos_emb = (rotary_pos_emb,) * 2
        # 此处类比transformer，生成q/k/v        
        # query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        # key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        # value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        query, key, value = self.get_query_key_value_tensors(hidden_states, key_value_states)

        key, value, rotary_pos_emb, attn_mask_type = self._adjust_key_value_for_inference(
            inference_params, key, value, rotary_pos_emb
        )

        if packed_seq_params is not None:
            query = query.squeeze(1)
            key = key.squeeze(1)
            value = value.squeeze(1)
        
        # 类比transformer完成rope变换
        if rotary_pos_emb is not None:
            q_pos_emb, k_pos_emb = rotary_pos_emb

            if packed_seq_params is not None:
                cu_seqlens_q = packed_seq_params.cu_seqlens_q
                cu_seqlens_kv = packed_seq_params.cu_seqlens_kv
            else:
                cu_seqlens_q = cu_seqlens_kv = None
            query = apply_rotary_pos_emb(
                query,
                q_pos_emb,
                config=self.config,
                cu_seqlens=cu_seqlens_q,
            )
            key = apply_rotary_pos_emb(
                key,
                k_pos_emb,
                config=self.config,
                cu_seqlens=cu_seqlens_kv,
            )

        # 类比完成transformer的attention_interface计算
        if self.checkpoint_core_attention and self.training:
            core_attn_out = self._checkpointed_attention_forward(
                query,
                key,
                value,
                attention_mask,
                attn_mask_type=attn_mask_type,
                packed_seq_params=packed_seq_params,
            )
        else:
            core_attn_out = self.core_attention(
                query,
                key,
                value,
                attention_mask,
                attn_mask_type=attn_mask_type,
                packed_seq_params=packed_seq_params,
            )

        if packed_seq_params is not None:
            core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)
        # 计算出output输出
        output, bias = self.linear_proj(core_attn_out)

        return output, bias
```

### 内容总结
如上，对transformer的主体模型结构与MindSpeed-LLM中的结构做了大体上的对应的模块映射比对讲解，有如下结论：

- transformer和MindSpeed-LLM中部分模块名称和结构设计相同，比如embedding，除去分布式和功能项设计，逻辑相同。
- 出于NPU亲和、框架不兼容以及性能优化需求，MindSpeed-LLM做了一些patch，这些部分与Transformer存在命名和设计的差异。
- transformer与MindSpeed-LLM存在对应但有差异的组件结构，比如此处的selfattention部分的设计，但核心逻辑都是如下公式：

![img_8.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/img_8.png)
- (本示例不涉及)也存在MindSpeed-LLM和transformer，模型结构和逻辑运行都对不上的新结构，这种就需要参考patch的方式进行适配。

模块适配的执行逻辑如下图所示：

![img_9.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/img_9.png)

针对本示例模型Qwen2.5-7B，通过模块实现比对总结出如下几点，适配要求：
- add-qkv-bias需要使能，disable-linear-bias不使能，完成权重和运行逻辑的对齐。
- normalization需要定义为RMSNorm，以确保PTNorm实际运行RMSNorm逻辑。
- position-embedding-type需要定义为rope， Transformer已开启该特性，确保位置编码对齐。
- rope-base（对应transformer default rope初始化中的rope_theta）需要与之相同，设置为1000000。
> 这里的rope_theta是由config.json定义的。

## 权重转换适配
![img_10.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/img_10.png)

- 已知获取到的权重文件为safetensors，与其配套文件safetensors.index.json，其中记录了权重的逐层张量块；
源文件中的config.json提供特性说明。
- 基于特性说明，完成safetensors中的权重模块加载并转成MindSpeed-LLM的数据存储模式，完成权重转换。

### 元数据解析
权重转换时，需要依赖transformer权重
在前述我们简要介绍了safetensors和safetensors.index.json，此处我们再详细展开说明。

#### safetensor.index.json与safetensors
权重在safetensors中以tensor的形式存在，整体是一个dict词典。

![img_11.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/img_11.png)

如图所示，左侧为词典中的权重key名称，说明该权重的所在层和权重名称，右边为该权重存储的safetensors的文件名，加载时会进行加载遍历，读取需要的tensor信息。
所有的权重名称都会在safetensor.index.json。不同的模型往往权重名称大同小异，我们要根据这里的权重key值，将权重tensor映射为MindSpeed-LLM的权重map，确保权重的加载读取。

#### config.json
权重转换依赖config.json的参量，下述对目标参量进行说明：

| 参数                           | 说明                                                       | 
|-------------------------------|------------------------------------------------------------|
| "model_type": "qwen2"         | 获取模型结构定义，以及权重在transformer的加载位置和shape等         | 
| "num_hidden_layers": 28       | 记录要从safetensors加载几层权重，以该模型为例，取值[0,28]          | 
| "num_key_value_heads": 4      | 说明使用了Group-Query-Attention， 其中kv_head是4个             | 
| "tie_word_embeddings": false  | 说明output和embedding的权重是否分割开，true则绑定，false则独立    | 
| "torch_dtype": "bfloat16"     | 说明权重以bf16格式记录数据                                      |

### MindSpeed-LLM的模型转换简介
模型转换的输入有两部分，执行脚本以及模型映射[model_cfg.json](../configs/checkpoint/model_cfg.json)，共同确定了模型转换时使用的模型特性。
基于模型映射config，确定目标模型的权重层映射关系，完成权重的加载和转换。
> 若少了模型特性，模型结构不完整，不再是目标模型。

#### model_cfg.json说明
```shell
    "base": {
      "config_set_value": {
        "seq_length": 4096,
        "global_batch_size": 1024,
        "add_position_embedding": true,
        "use_rotary_position_embeddings": true,
        "add_bias_linear": false,
        "gradient_accumulation_fusion": false,
        "normalization": "RMSNorm",
        "swiglu": true,
        "tokenizer_type": "Llama2Tokenizer",
        "group_query_attention": false,
        "qkv_type": "unpack",
        "fc_type": "gate_up_down"
      },
      "config_hf_key_mapping": {
        "max_position_embeddings": "max_position_embeddings",
        "hidden_size": "hidden_size",
        "num_attention_heads": "num_attention_heads",
        "num_layers": "num_hidden_layers",
        "num_key_value_heads": "num_key_value_heads",
        "vocab_size": "vocab_size",
        "intermediate_size": "intermediate_size",
        "norm_epsilon": "rms_norm_eps",
        "tie_word_embeddings": "tie_word_embeddings",
		    "torch_dtype": "torch_dtype"
      },
      "model_hf_key_mapping": {
        "model": "module[0]",
        "embedding_word_embeddings": "model.embed_tokens",
        "embedding_word_embeddings_norm": "model.embedding.word_embeddings.norm",
        "layers": "model.layers",
        "layers_input_layernorm": "model.layers[layer_idx].input_layernorm",
        "layers_self_attention_linear_proj": "model.layers[layer_idx].self_attn.o_proj",
        "layers_self_attention_linear_q_proj": "model.layers[layer_idx].self_attn.q_proj",
        "layers_self_attention_linear_k_proj": "model.layers[layer_idx].self_attn.k_proj",
        "layers_self_attention_linear_v_proj": "model.layers[layer_idx].self_attn.v_proj",
        "layers_self_attention_pre_mlp_layernorm": "model.layers[layer_idx].post_attention_layernorm",
        "layers_mlp_gate_proj": "model.layers[layer_idx].mlp.gate_proj",
        "layers_mlp_up_proj": "model.layers[layer_idx].mlp.up_proj",
        "layers_mlp_linear_fc2": "model.layers[layer_idx].mlp.down_proj",
        "final_layernorm": "model.norm",
        "output_layer": "lm_head",
        "rm_head": "score"
      }
    }
```
如上所示为截取的一个base model-type的映射关系表。
- config_set_value: 记录模型组件定义及初始化依赖特性，如图中的use_rotary_position_embeddings少了会使模型运行逻辑变化，不再是原先模型。
  - qkv_type为unpack： qkv权重被堆叠打包分为一个权重，查看safetensor.index.json，其中的q/k/v_proj都是独立的权重，可知无打包。常见打包方式有gqa、mla等。
  - fc_type为gate_up_down：mlp部分的类别，分为[h_to_4h,gate_up_down]，由模型结构确定，可从safetensor.index.json确认。
- config_hf_key_mapping:模型特性参数名称映射关系，不同模型参数名定义有区别，这里要将他们完成参量统一，右边为transformer config.json的对应参数。
- model_hf_key_mapping:模型权重层组映射关系，左边是MindSpeed-LLM定义的模型权重层名称，右边是transformer的safetensor.index.json中的名称。
> 如果是新特性，就需要自己适配，添加一个特性参量，并在base model-type中填写默认值，以保证不影响其他模型的转换。

如下图为Qwen2.5-7B适配模型的model-type映射关系表，即为base model-type的映射关系。
```shell
    "llama2": {
      "__base__": "base"
    }
```
根据我们前面分析的点：
需要：
- add_qkv_bias： 通过执行脚本特性引入。
- add_position_embedding： 已适配在Config_set_value中，赋值为true
- use_rotary_position_embeddings： 已适配在Config_set_value中，赋值为true
- "normalization": "RMSNorm"： 已适配在Config_set_value中，赋值为true
不需要：
- add_bias_linear：已适配在Config_set_value中，赋值为false

#### 权重转换适配代码讲解
以Qwen2.5-7B模型为例，假若进行hf2mcore的权重转换，配置的执行脚本如下所示：
```shell
python convert_ckpt.py \
       --use-mcore-models \
       --model-type GPT \
       --load-model-type hf \
       --save-model-type mg \
       --target-tensor-parallel-size 1 \
       --target-pipeline-parallel-size 4 \
       --add-qkv-bias \         # 关键参数
       --load-dir ./model_from_hf/qwen2.5_7b_hf/ \
       --save-dir ./model_weights/qwen2.5_mcore/ \
       --tokenizer-model ./model_from_hf/qwen2.5_7b_hf/tokenizer.json \
       --model-type-hf llama2 \
       --params-dtype bf16 
```
| 参数                                | 说明                                   |
|-----------------------------------|--------------------------------------|
| `--model-type GPT`                | 指定模型类型为GPT系列                         |
| `--use-mcore-models`              | 转换为Megatron-Mcore格式                  |
| `--target-tensor-parallel-size`   | 张量并行度设置                              |
| `--target-pipeline-parallel-size` | 流水线并行度设置                             |
| `--tokenizer-model`               | 指定分词器路径                              |
| `--load-model-type`               | 加载权重的类别（可以是hf、mg）                    |
| `--save-model-type`               | 存储权重的类别（可以是hf、mg）                    |
| `--load-dir`                      | 权重文件加载路径                             |
| `--save-dir`                      | 权重文件保存路径                             |
| `--model-type-hf`                 | huggingface模型类别，默认为llama2            |
| `--params-dtype`                  | 权重转换的数据格式，默认为fp16，若源文件为bf16，则设置为bf16 |
| `--add-qkv-bias`                  | q/k/v增加bias部分加载和转换                   |

已知入口函数为convert_ckpt.py，如下图为权重转换的流程图，

![img_12.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/img_12.png)

下面对代码进行精要解读：
```shell
# 主进程函数说明
def main():
    # 参量载入，根据运行脚本中的特性参量，获取配置信息
    parser = argparse.ArgumentParser(description="Megatron Checkpoint Utility Arguments",
                                     allow_abbrev=False, conflict_handler='resolve')
    ...
    
    
    if known_args.load_model_type == 'optim':
        ...
    else:
        # 对于hf2mocre和mcore2hf采用该分支
        use_saver = known_args.load_model_type is None
        # 根据loader参量，通过load_plugin函数进行路径构造，尝试import目标类，完成loader和saver对象的构建。
        if use_saver:
            loader = load_plugin('loader', known_args.loader)
            saver = load_plugin('saver', known_args.saver)
        else:
            loader = load_plugin('loader', known_args.load_model_type)
            saver = load_plugin('saver', '')
        # 完成loader和saver对象从执行脚本参量导入
        loader.add_arguments(parser)
        saver.add_arguments(parser)

        args = parser.parse_args()
        
        # 构建独立进程saver_poc，本主线程进行权重加载并传输切块权重给到saver，通过queue进行数据沟通。
        queue = mp.Queue(maxsize=args.max_queue_size)
        ...
        logger.info("Starting saver...")
        # 独立分支执行save_model_checkpoint的进程，后续进行展开说明。
        saver_proc = mp.Process(target=saver.save_model_checkpoint, args=(model_provider, queue, args))
        saver_proc.start()

        # loader完成权重的加载，后续进行展开说明
        logger.info("Starting loader...")
        loader.load_checkpoint(model_provider, queue, args)

        logger.info("Waiting for saver to complete...")
        saver_proc.join()
```

```shell
# loader.load_checkpoint, 见mindspeed_llm/tasks/checkpoint/loader_hf.py，mcore分支的load_checkpoint请自行学习。
def _load_checkpoint(model_provider, queue, args):
    # 环境和输入校验
    ...

    # 创建并初始化hf_model, 初始化时从执行脚本载入参量，并根据开源模型config.json获取参量
    # 同时根据model-type-hf从model_cfg.json获取参量映射关系和特性以及权重映射关系
    model_hf = get_huggingface_model(args)
    args_hf = model_hf.get_args()
    
    ...
    # 创建并初始化model_mg，根据执行脚本以及model-type-hf从model_cfg.json获取参量映射关系和特性以及权重映射关系
    model_mg = get_megatron_model(model_provider, args_cmd=args)
    # 根据hf_args进行参量更新
    model_mg.initialize_megatron_args(args_hf, queue)

    # 完成切分配置以及参量设置
    model_mg.set_tensor_model_parallel_world_size(model_mg.args.tensor_model_parallel_size)
    model_mg.set_expert_model_parallel_world_size(model_mg.args.expert_model_parallel_size)
    
    ...
    # model_hf加载safetensors权重，有权重和结构
    model_hf.get_modules_from_pretrained()
    # model_mg从配置加载模型并初始化模型结构，无权重
    model_mg.get_modules_from_config()
    # 从model_hf将权重搬到model_mg上，此处的一个关键函数是set_layer_state_base。
    model_mg.update_module(model_hf)

    def queue_put(name, msg):
        logger.info(f"sending {name}")
        msg["name"] = name
        queue.put(msg)

    # 将token_embedding部分的权重转化送入queue，给到saver
    # get_message_xxx函数都是通过构造实现，后续说明。
    message = get_message_preprocess(model_mg, md)
    queue_put("embeddings", message)

    # 依层，将norm\attn\mlp权重切分转为MindSpeed-LLM接口的权重message送入queue
    for layer_idx in range(margs.num_layers):
        message = {}
        message = get_message_layer_norm(message, model_mg, layer_idx, md, args)
        message = get_message_layer_attn(message, model_mg, layer_idx, md, args)
        message = get_message_layer_mlp(message, model_mg, layer_idx, md)
        to_detach(message)
        queue_put(f"transformer layer {layer_idx}", message)

    ...
    # 待所有权重送入后，发送结束message
    queue.put("done")
```
由于权重层命名规则统一，为减少重复工作量，通过构造函数方式完成大量set\get\has等weight\bias函数的建构。
- 比如：get_embedding_word_embeddings_norm_weight
- 比如：get_embedding_word_embeddings_norm_bias
- 比如：set_layers_mlp_gate_proj_weight
- 比如：set_layers_mlp_gate_proj_bias
- 比如：has_embedding_word_embeddings_norm_module
此类函数的构建，可以查看mindspeed_llm/tasks/checkpoint/models.py::ModelBase::__register_functions。

```shell
# set_layer_state_base补充说明
# 该函数的作用是将hf对应层的权重转化为megatron-core格式的权重。
# 其中有部分权重加载时要分情况处理，比如我们之前提到的qkv_type，fc_type，如下以qkv_type做简要说明
# 见set_layer_state_base调用的set_attn_state的src_model.get_layers_self_attention_linear_qkv_weight
def __get_layers_self_attention_linear_qkv_module(self, layer_idx=0):
    ...
    
    # 此处根据权重的不同将其分成了各种type，由实际safetensors中数据堆叠形式决定，如下解读代码可完成堆叠格式反推。
    qkv_type = self.args.qkv_type
    if qkv_type == "unpack":
        if hasattr(self.args, 'cla_share_factor') and layer_idx % self.args.cla_share_factor == 1:
            q_proj = self.get_layers_self_attention_linear_q_proj_module(layer_idx=layer_idx)
            query_key_value_weight = q_proj.weight
            self.layers_self_attention_linear_qkv_caches["weight"] = query_key_value_weight
        else:
            q_proj = self.get_layers_self_attention_linear_q_proj_module(layer_idx=layer_idx)
            k_proj = self.get_layers_self_attention_linear_k_proj_module(layer_idx=layer_idx)
            v_proj = self.get_layers_self_attention_linear_v_proj_module(layer_idx=layer_idx)
            query_key_value_weight = [q_proj.weight, k_proj.weight, v_proj.weight]
            query_key_value_bias = [q_proj.bias, k_proj.bias, v_proj.bias]
            self.layers_self_attention_linear_qkv_caches["weight"] = (
                qkv_concatenate_weight(query_key_value_weight))
            if self.args_cmd.add_qkv_bias:
                self.layers_self_attention_linear_qkv_caches["bias"] = (qkv_concatenate_bias(query_key_value_bias))
    elif qkv_type == "pack_mla":
        q_proj = self.get_layers_self_attention_linear_q_proj_module(layer_idx=layer_idx)
        kv_proj = self.get_layers_self_attention_linear_kv_proj_module(layer_idx=layer_idx)
        query_key_value_weight = [q_proj.weight.reshape((-1, self.args.hidden_size)),
                                  kv_proj.weight.reshape((-1, self.args.hidden_size))]
        self.layers_self_attention_linear_qkv_caches["weight"] = (torch.cat(query_key_value_weight, dim=0))
        if self.args_cmd.add_qkv_bias:
            query_key_value_bias = [q_proj.bias, kv_proj.bias]
            self.layers_self_attention_linear_qkv_caches["bias"] = (qkv_concatenate_bias(query_key_value_bias))
    elif qkv_type == "pack_gqa":
        qkv_pack = self.get_layers_self_attention_linear_qkv_pack_module(layer_idx=layer_idx)
        qkv_pack_weight = qkv_pack.weight
        full_q = dim * nh
        end_k = full_q + ng * dim
        q_weight = qkv_pack_weight[:full_q, :]
        k_weight = qkv_pack_weight[full_q:end_k, :]
        v_weight = qkv_pack_weight[end_k:, :]
        query_key_value_weight = [q_weight, k_weight, v_weight]
        self.layers_self_attention_linear_qkv_caches["weight"] = (qkv_concatenate_weight(query_key_value_weight))
        if self.args_cmd.add_qkv_bias:
            qkv_pack_bias = qkv_pack.bias
            q_bias = qkv_pack_bias[:full_q]
            k_bias = qkv_pack_bias[full_q:end_k]
            v_bias = qkv_pack_bias[end_k:]
            query_key_value_bias = [q_bias, k_bias, v_bias]
            self.layers_self_attention_linear_qkv_caches["bias"] = (qkv_concatenate_bias(query_key_value_bias))
    elif qkv_type == "pack_self":
        qkv_pack = self.get_layers_self_attention_linear_qkv_pack_module(layer_idx=layer_idx)
        qkv_pack_weight = qkv_pack.weight
        self.layers_self_attention_linear_qkv_caches["weight"] = qkv_pack_weight
        if self.args_cmd.add_qkv_bias:
            qkv_pack_bias = qkv_pack.bias
            full_q = dim * nh
            end_k = full_q + ng * dim
            q_bias = qkv_pack_bias[:full_q, :]
            k_bias = qkv_pack_bias[full_q:end_k, :]
            v_bias = qkv_pack_bias[end_k:, :]
            query_key_value_bias = [q_bias, k_bias, v_bias]
            self.layers_self_attention_linear_qkv_caches["bias"] = (qkv_concatenate_bias(query_key_value_bias))        
    else:
        raise ValueError(f"Unsupported types. {qkv_type}")
```

下面对saver对象的代码做解析进行说明。
```shell
# saver.save_model_checkpoint, 见mindspeed_llm/tasks/checkpoint/saver.py
def save_model_checkpoint(model_provider, queue, args):
    # 若干先验
    ...

    def queue_get(name=None):
        val = queue.get()
        if val == "exit":
            logger.error("Loader exited, exiting saver")
            exit(1)
        if name is not None and args.checking and val["name"] != name:
            val_name = val["name"]
            logger.error(f'Unexpected message. Expecting "{name}" but got "{val_name}". Exiting saver.')
            exit(1)
        if name is not None:
            logger.info(f"received {name}")
        return val

    ...

    md = queue_get()
    ...

    # 创建并初始化model_mg, 根据执行脚本以及model-type-hf从model_cfg.json获取参量映射关系和特性以及权重映射关系
    model_mg = get_megatron_model(model_provider=model_provider, args_cmd=args, md=md)
    model_mg.initialize_megatron_args(queue=queue, saver_megatron=True)

    # MindSpeed-LLM是面向分布式的大模型训练框架，根据并行策略（流水线并行、张量并行、专家并行）分配模型分片完成并行配置，并完成模块初始化
    mpu.set_pipeline_model_parallel_rank(0)
    post_process = args.target_pipeline_parallel_size == 1
    update_padded_vocab_size(md, model_mg, model_mg.args.vocab_size)
    model_mg.get_modules_from_config(pp_stage_cache_flag=True)

    # 根据embedding关键词从queue获取对应msg，根据张量并行，将embedding权重按照维度切分并分配到不同设备
    embeddings_msg = queue_get("embeddings")
    out_word_embed_list = set_model_preprocess(model_mg, embeddings_msg)
    check_message(embeddings_msg)
    margs = model_mg.get_args()

    ...
    # noop_layers为插空层功能，非常规权重转换方法，ignore
    if args.noop_layers:
        args.noop_layers = args.noop_layers.split(',')
        args.noop_layers = [int(i) for i in args.noop_layers]
    
    # 根据虚拟流水并行vpp和流水并行，逐层进行模块初始化，并获取norm\attn\mlp message 进行加载
    for vp_rank in range(virtual_pipeline_model_parallel_size):
        model_mg.set_virtual_pipeline_model_parallel_rank(vp_rank)
        kwargs = {"vp_rank": vp_rank}
        for pp_rank in range(args.target_pipeline_parallel_size):
            # For later pipeline parallel ranks, make the new models
            mpu.set_pipeline_model_parallel_rank(pp_rank)
            model_mg.get_modules_from_config(pp_stage_cache_flag=True)
            kwargs["pp_rank"] = pp_rank
            for layer in range(len(model_mg.get_layers_module())):
                kwargs["layer_idx"] = layer
                msg = queue_get(f"transformer layer {total_layer_num}")
                set_model_layer_norm(model_mg, msg, md, **kwargs)
                set_model_layer_attn(model_mg, msg, md, **kwargs)
                set_model_layer_mlp(model_mg, msg, md, total_layer_num, **kwargs)

                total_layer_num = total_layer_num + 1
                # For noop layers, we dont check keys.
                check_message(msg)

            post_process = (
                    (pp_rank == args.target_pipeline_parallel_size - 1) &
                    (vp_rank == virtual_pipeline_model_parallel_size - 1)
            )
            if post_process:
                msg = queue_get("final norm")
                set_model_postprocess(model_mg, msg, md, out_word_embed_list, **kwargs)
                check_message(msg)

                if md.output_layer:
                    msg = queue_get("output layer")
                    set_model_output_layer(model_mg, msg, md, **kwargs)
                    check_message(msg)

                if md.rm_head:
                    msg = queue_get("rm head")
                    set_model_rm_head(model_mg, msg, md, **kwargs)
                    check_message(msg)

            if vp_rank == virtual_pipeline_model_parallel_size - 1:
                save_model(model_mg, md, **kwargs)
    logger.info("Done!")
```

## 精度调试
通过上述两章内容的介绍，我们完成了模型适配和权重转换的功能打通，下面对整个适配工作进行精度验证，确保适配成功，整体验证分为如下几步：
- 脚本可以正常运行，完成hf2mcore和mcore2hf的权重转换，功能可逆。
- 通过hf->mcore->hf，转换得到的transformer权重与原始权重通过张量比对，确认结果可逆。
- 通过单步推理验证模型权重适配前向对齐。
- 完成预训练脚本编写，可执行预训练任务，通过多epoch loss曲线比对确认功能正常。

### 权重转换功能可逆验证
在权重适配阶段，完成hf2mcore的适配工作，编写一个mcore2hf的权重转换脚本。
```shell
python convert_ckpt.py \
    --use-mcore-models \
    --model-type GPT \
    --model-type-hf llama2 \
    --load-model-type mg \
    --save-model-type hf \
    --target-tensor-parallel-size 1 \
    --target-pipeline-parallel-size 1 \
    --add-qkv-bias \
    --load-dir ./model_weights/qwen2.5_mcore/ \
    --save-dir ./model_from_hf/qwen2.5_7b_hf/  # 需要填入原始HF模型路径，新权重会存于./model_from_hf/qwen2.5_7b_hf/mg2hg/
```
| 参数                                | 说明                                   |
|-----------------------------------|--------------------------------------|
| `--model-type GPT`                | 指定模型类型为GPT系列                         |
| `--use-mcore-models`              | 输入权重为Megatron-Mcore格式                |
| `--target-tensor-parallel-size`   | 张量并行度设置，transformer权重的tp为1           |
| `--target-pipeline-parallel-size` | 流水线并行度设置，transformer权重的pp为1          |
| `--load-model-type`               | 加载权重的类别（可以是hf、mg）                    |
| `--save-model-type`               | 存储权重的类别（可以是hf、mg）                    |
| `--load-dir`                      | 权重文件加载路径                             |
| `--save-dir`                      | 权重文件保存路径                             |
| `--model-type-hf`                 | huggingface模型类别，默认为llama2            |
| `--add-qkv-bias`                  | q/k/v增加bias部分加载和转换                   |

如果脚本可以运行，完成hf2mcore和mcore2hf的权重转换，可以认为功能上已完成了适配，可以进行权重转换。

### 权重转换结果可逆验证
如果转换正确，那么权重信息不会有丢失，此时我们通过hf->mcore->hf，获取transformer权重可以与开源源数据做比对，张量应该是完全一致的。
```shell
import os
from safetensors.torch import load_file

def read_safetensors_files(directory):
    all_weights = {}

    # 遍历指定目录下的所有safetensors文件并加载
    for filename in os.listdir(directory):
        if filename.endswith(".safetensors"):
            filepath = os.path.join(directory, filename)
            weights = load_file(filepath)

            # 将当前文件的权重添加到总的字典中
            for key, value in weights.items():
                all_weights[key] = value

    return all_weights

# 加载原始权重
ori_hf_path = "/home/hf_weights/Qwen2.5-7B/"  # 替换为你的文件夹路径
ori_weights_dict = read_safetensors_files(ori_hf_path)
print(f"origin_weight block num:{len(ori_weights_dict.keys())}")
# 加载转换后权重
trans_hf_path = "/home/hf_weights/Qwen2.5-7B/mg2hf/"
trans_weights_dict = read_safetensors_files(ori_hf_path)
print(f"trans_weight block num:{len(trans_weights_dict.keys())}")

# 权重比较，如果有任一块权重张量的任一值不对应则报false
for key in ori_weights_dict.keys():
    print(f"weights_dict[{key}]:{(ori_weights_dict[key] == trans_weights_dict[key]).all()}")
```
如上提供比对两份hf权重的方法，如果完全对应，则该模块张量比对结果反馈为true。

![img.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/weight_map.png)

### 前向验证
精度对齐三要素：模型输入一致、模型结构一致、模型权重一致。
- 模型适配确保结构一致；
- 权重转换适配确保权重相同；
- 手动设置相同的输入，确保输入一致。
以上动作建议都在NPU上运行，以排除GPU和NPU硬件架构带来的精度误差影响。

![img_1.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/input.png)

#### 获取transformer前向结果
```shell
# transformer前向结果计算，通过构造输入，加载transformer原始权重，获得推理结果
import torch
import torch_npu
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
from torch_npu.contrib import transfer_to_npu

load_dir = "/home/hf_weights/Qwen2.5-7B"

def set_device(device_id):
    torch.npu.set_device(torch.device(f"npu:{device_id}"))

def load_model():
    """load model"""
    tokenizer = AutoTokenizer.from_pretrained(load_dir, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(load_dir, trust_remote_code=True).npu().eval()
    return tokenizer, model

def logits_output(model):
    """forward"""
    input_ids = torch.tensor([i for i in range(10000,12048)]).unsqueeze(0).npu()
    attention_mask = torch.ones(tuple(input_ids.shape), dtype=int)
    with torch.no_grad():
        logits = model.forward(
            input_ids=input_ids,
            position_ids=None,
            attention_mask=attention_mask.npu(),
        )
    print("logits Shape:", logits.logits.shape)
    print("logits: ", logits.logits)
    np.save("./gpu_forward_out_hf_qwen25_7b_logits_fp16.npy", logits.logits.float().cpu().numpy())

if __name__ == "__main__":
    set_device(1)
    tokenizer, model = load_model()
    logits_output(model)
```

#### 获取MindSpeed-LLM前向结果

```shell
# MindSpeed-LLM前向结果计算，通过修改inference.py的main函数，构造输入，加载转换后的mcore权重，获得推理结果
# 如下为修改后inference.py的main函数
def main():
    initialize_megatron(args_defaults={'no_load_rng': True,
                                       'no_load_optim': True})

    args = get_args()

    model = MegatronModuleForCausalLM.from_pretrained(
        model_provider=model_provider,
        pretrained_model_name_or_path=args.load
    )

    # 生成指定输入
    import torch
    import numpy as np
    from megatron.training.utils import get_ltor_masks_and_position_ids
    input_ids = torch.tensor([i for i in range(10000, 12048)]).unsqueeze(0).npu()
    eod = 0
    reset_position_ids = False
    reset_attention_mask = False
    eod_mask_loss = False
    attention_mask, loss_mask, _ = get_ltor_masks_and_position_ids(
        input_ids,
        eod,
        reset_position_ids,
        reset_attention_mask,
        eod_mask_loss)
    with torch.no_grad():
        outputs = model.forward(input_ids=input_ids, position_ids=None, attention_mask=attention_mask.npu())
        print(outputs.shape, outputs.dtype)
        np.save("./npu_forward_out_hf_qwen25_7b_logits_fp16.npy", outputs.cpu().numpy())
```
完成代码修改后，还需要配合一个脚本来配合启动获取结算结果[generate_qwen25_7b_ptd.sh](../examples/mcore/qwen25/generate_qwen25_7b_ptd.sh)。
```shell
# 修改脚本如下内容
# 填入mcore权重路径以及源文件tokenizer路径
CHECKPOINT="your model directory path"
TOKENIZER_PATH="your tokenizer directory path"

# Change for multinode config
MASTER_ADDR=localhost
MASTER_PORT=6000
NNODES=1
NODE_RANK=0
NPUS_PER_NODE=8
WORLD_SIZE=$(($NPUS_PER_NODE*$NNODES))

# 根据转换权重切分配置tp和pp，seq_length修改为2048，匹配构造输入
TP=4
PP=2
SEQ_LENGTH=2048
```
此时获取了两个运行结果，基于如下脚本进行余弦相似度验证。
```shell
import torch
import numpy as np

def print_all(text, x):
    print(text, np.max(x.reshape(-1)), np.min(x.reshape(-1)), np.mean(x.reshape(-1)),
          np.std(x.reshape(-1)))

def cos_sim(x, y):
    x = torch.tensor(x)
    y = torch.tensor(y)
    x = x / torch.norm(x, dim=-1, keepdim=True)
    y = y / torch.norm(y, dim=-1, keepdim=True)
    s = x * y
    s = s.sum(-1)
    return s.numpy()

if __name__ == "__main__":
    x = np.load("./npu_forward_out_hf_qwen25_7b_logits_fp16.npy").astype("float32")
    y = np.load("./gpu_forward_out_hf_qwen25_7b_logits_fp16.npy").astype("float32")
    d = np.abs(x - y)
    s = cos_sim(x, y)

    print_all("x_data:", x)
    print_all("y_data:", y)
    print_all("abs_delta:", d)
    print(f"cos_sim.shape:{x.shape}")
    print_all("cos_sim_result:", s)
```
结算得到的两个输出结果的相似度，余弦相似值应该>98%以上，认为前向对齐。
> 如果对不齐怎么办？请见附录，断点模块排查法

![img_2.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/cossim.png)

### 预训练迭代验证
通过以上验证，确认前向方案已对齐，还需要通过迭代来确认反向和迭代是否对齐。需要在NPU/GPU上分别编写预训练脚本，在保障开启特性和输入相同的情况下，检查是否loss曲线可以正常对齐。这里介绍NPU部分要如何编写脚本获取loss曲线，教程不涉及GPU部分。

如下对所使用脚本做参量说明：
```shell
#!/bin/bash

...
# 填写转好的权重和数据集，并保证NPU和GPU的权重和数据集相同
CKPT_LOAD_DIR="your model ckpt path"
CKPT_SAVE_DIR="your model save ckpt path"
DATA_PATH="your data path"
TOKENIZER_PATH="your tokenizer path"

# 与转好的权重对应，建议为tp1pp4
TP=1
PP=4
SEQ_LEN=4096
MBS=1
GBS=32

DISTRIBUTED_ARGS="
    --nproc_per_node $NPUS_PER_NODE \
    --nnodes $NNODES \
    --node_rank $NODE_RANK \
    --master_addr $MASTER_ADDR \
    --master_port $MASTER_PORT
"

GPT_ARGS="
    # 运行megatron-core model分支
    --use-mcore-models \
    
    # 切分配置
    --tensor-model-parallel-size ${TP} \
    --pipeline-model-parallel-size ${PP} \
    --sequence-parallel \
    
    # 模型配置，开启后才是该模型
    --num-layers 28  \
    --hidden-size 3584  \
    --ffn-hidden-size 18944 \
    --num-attention-heads 28  \
    --disable-bias-linear \
    --add-qkv-bias \
    --group-query-attention \
    --num-query-groups 4 \
    --normalization RMSNorm \
    --use-flash-attn \
    --swiglu \
    --position-embedding-type rope \
    --rotary-base 1000000 \
    --untie-embeddings-and-output-weights \
    --padded-vocab-size 152064 \
    --norm-epsilon 1e-6 \
    --bf16 \
    --tokenizer-type PretrainedFromHF \
    --tokenizer-name-or-path ${TOKENIZER_PATH} \
    
    # 优化加速
    --use-fused-swiglu \
    --use-fused-rmsnorm \
    --use-fused-rotary-pos-emb \

    --max-position-embeddings ${SEQ_LEN} \
    --seq-length ${SEQ_LEN} \
    --micro-batch-size ${MBS} \
    --global-batch-size ${GBS} \
    --make-vocab-size-divisible-by 1 \
    --no-gradient-accumulation-fusion \
    --no-masked-softmax-fusion \
    --attention-softmax-in-fp32 \
    
    # 超调参量
    --attention-dropout 0.0 \
    --hidden-dropout 0.0 \
    --train-iters 2000 \
    --lr 1.25e-6 \
    --lr-decay-style cosine \
    --min-lr 1.25e-7 \
    --lr-warmup-fraction 0.01 \
    --init-method-std 0.01 \
    --weight-decay 1e-1 \
    --clip-grad 1.0 \
    --adam-beta1 0.9 \
    --adam-beta2 0.95 \
    --initial-loss-scale 4096 \
"

...
torchrun $DISTRIBUTED_ARGS pretrain_gpt.py \
    $GPT_ARGS \
    $DATA_ARGS \
    $CKPT_ARGS \
    $OUTPUT_ARGS \
    --distributed-backend nccl \
    | tee logs/pretrain_mcore_qwen25_7b_32k.log
```

获取NPU的运行日志后，将其loss曲线进行绘制比对如下图所示：

![img_4.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/loss_line.png)

对齐结论：Loss曲线差异趋向收敛；gbs32跑2000步loss RME<1%。
> 如果对不齐怎么办？请见附录，hook挂载比对正反向误差定位方法

## 附录
### 断点模块排查法
如果输出结果不满足要求，需要模型结构同一位置插入断点进行结果比对定位问题，去细致比对输入input和weight是否相同，计算结果是否相同。

![img_3.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/breakpoint.png)

如图所示为一个Attention的断点插入方法。
通过前述我们把transformer开源模型框架与MindSpeed-LLM框架进行模块级对齐，据此我们可以找到模块的对应关系，从而进行断点调测。两份运行代码分别可见：
- transformers/src/transformers/models/qwen2/modeling_qwen2.py
> 该路径为相对路径，请确认你环境中的transformer找到对应的modeling_qwen2.py。
- MindSpeed-LLM/megatron/core/transformer/attention.py
> 完成环境安装后，应有一份megatron代码放置在MindSpeed-LLM下。

断点调测方法：
在相同位置加入import pdb;pdb.set_trace()，从而进行debug，确认逐层的输入是否已知，权重是否相同，结果误差等。
或者也可以通过print()，将需要的参量打印出来进行调试。

### hook挂载比对正反向误差定位方法
对于多线程来说，通过pdb进行断点定位，多有不便，通过hook方法可以将正反向中间结果答应出来，便于比对定位，下面以MindSpeed-LLM为例进行说明，在NPU架构打hook获取中间运行结果。
```shell
# 修改pretrain_gpt.py, 增加如下定义函数
import torch
from megatron.core.transformer.identity_op import IdentityOp

def com(tensor, file_path):
    ab = torch.abs(tensor)
    with open(file_path, 'a') as file:
        try:
            print(">sum:, %e" % torch.sum(ab).item(), file=file)
        except:
            print("This tensor do not support sum and abs!", file=file)
        try:
            print(">mean:, %e" % torch.mean(ab).item(), file=file)
        except:
            print("This tensor do not support mean!", file=file)
        try:
            print(">max:, %e" % torch.max(ab).item(), file=file)
        except:
            print("This tensor do not support max!", file=file)
        try:
            print(">min:, %e" % torch.min(ab).item(), file=file)
        except:
            print("This tensor do not support min!", file=file)
        file.close()

def print_tensor(tensors, file_path):
    if tensors is None:
        return

    with open(file_path, 'a') as file:
        if isinstance(tensors, torch.Tensor) or isinstance(tensors, tuple) or isinstance(tensors, list):
            if isinstance(tensors, tuple) or isinstance(tensors, list):
                for tensor in tensors:
                    if tensor is None:
                        continue
                    if isinstance(tensor, torch.Tensor):
                        tensor = tensor.detach().to(torch.float32)
                        print(tensor.shape, tensor.dtype, file=file)
                        print(tensor, file=file)
                        com(tensor, file_path)
                    else:
                        if isinstance(tensor, tuple) or isinstance(tensor, list):
                            print_tensor(tensor, file_path)
                        else:
                            try:
                                tensor = tensor.detach().to(torch.float32)
                                print(tensor.shape, tensor.dtype, file=file)
                                print(tensor, file=file)
                                com(tensor, file_path)
                            except:
                                print(file=file)
                                # print(name, tensor, file=file)
            else:
                tensors = tensors.detach().to(torch.float32)
                print(tensors.shape, tensors.dtype, file=file)
                com(tensors, file_path)
        else:
            print(type(tensors), file=file)
            # print(tensors, file=file)
        file.close()

def hook_func(name, module):
    def hook_function(module, inputs, outputs):
        if torch.distributed.get_rank() == 0:
            file_path = 'npu_hook_0rank_i.txt'
            with open(file_path, 'a') as file:
                print(f"--------------------------[{name}]--------------------------------", file=file)
                print("--------------------------input--------------------------------", file=file)
                file.close()
            print_tensor(inputs, file_path)

        if torch.distributed.get_rank() == 0:
            file_path = 'npu_hook_0rank_o.txt'
            with open(file_path, 'a') as file:
                print("---------------------------output------------------------------", file=file)
                file.close()
            print_tensor(outputs, file_path)

    return hook_function

def hook_for_model(model):
    for name, module in model.named_modules():
        is_identity_module = isinstance(module, IdentityOp)
        is_zero_dropout = isinstance(module, torch.nn.modules.dropout.Dropout) and module.p < 1e-6
        is_useful_module = not is_identity_module and not is_zero_dropout
        is_atom_module = len(list(module.children())) == 0
        if is_useful_module and is_atom_module:
            module.register_forward_hook(hook_func('[forward]: ' + name, module))
            module.register_backward_hook(hook_func('[backward]: ' + name, module))
            
# 往def model_provider(pre_process=True, post_process=True)：函数下
    ...
        model = megatron.legacy.model.GPTModel(
            config,
            num_tokentypes=0,
            parallel_output=True,
            pre_process=pre_process,
            post_process=post_process
        )
    # 完成model构建后，return之前挂载hook
    hook_for_model(model)
    return model
    
# 单前向验证
def forward_step(data_iterator, model: GPTModel):
    ...
    exit() # 跑一次前向即退出，否则会一直运行，可等出第二步迭代后kill，即可获取完整正反向。
    return
```
运行预训练脚本，采集正反向中间层数据。如下图所示，每层的组成模块都有对应的输出，就可以进行比对，从而定位确认问题点，再进行排查修复。

![img_5.png](https://gitee.com/ascend/MindSpeed-LLM/raw/master/sources/images/model_adaptation/hook.png)

